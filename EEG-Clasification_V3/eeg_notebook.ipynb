{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "from scipy import signal as sig\n",
    "from sklearn.decomposition import FastICA\n",
    "import pywt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output directory and create it if it doesn't exist\n",
    "output_dir = os.path.join(os.getcwd(), '@Output')\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created output directory: {output_dir}\")\n",
    "\n",
    "# Helper function to join path with output directory\n",
    "def get_output_path(filename):\n",
    "    return os.path.join(output_dir, filename)\n",
    "\n",
    "# Load the .mat file\n",
    "mat_data = sio.loadmat('exportData.mat')\n",
    "\n",
    "fs_eeg = 125\n",
    "epoch_length = 30 #in second\n",
    "\n",
    "# Create eeg_data with eeg1 and eeg2\n",
    "eeg_data = np.zeros((2, len(mat_data['eeg1'][0])))\n",
    "eeg_data[0, :] = mat_data['eeg1'][0]\n",
    "eeg_data[1, :] = mat_data['eeg2'][0]\n",
    "\n",
    "# Create eog_data with eog1 and eog2\n",
    "eog_data = np.zeros((2, len(mat_data['eogl'][0])))\n",
    "eog_data[0, :] = mat_data['eogl'][0]\n",
    "eog_data[1, :] = mat_data['eogr'][0]\n",
    "\n",
    "# Create emg_data with one emg channel, \"emg\"\n",
    "emg_data = np.zeros((1, len(mat_data['emg'][0])))\n",
    "emg_data[0, :] = mat_data['emg'][0]\n",
    "\n",
    "# Create emg_data with one ecg channel, \"ecg\"\n",
    "ecg_data = np.zeros((1, len(mat_data['ecg'][0])))\n",
    "ecg_data[0, :] = mat_data['ecg'][0]\n",
    "\n",
    "# Simple plot of the two EEG channel at half of the dataset for 30 seconds\n",
    "# in two different subplots\n",
    "fig, axs = plt.subplots(2, 1)\n",
    "axs[0].plot(eeg_data[0, :fs_eeg*epoch_length*30])\n",
    "axs[1].plot(eeg_data[1, :fs_eeg*epoch_length*30])\n",
    "plt.show()\n",
    "\n",
    "# filter the eeg_data with a bandpass filter between 0.5 and 45 Hz\n",
    "nyquist = fs_eeg / 2\n",
    "low_cutoff = 0.5 / nyquist\n",
    "high_cutoff = 45 / nyquist\n",
    "b, a = sig.butter(4, [low_cutoff, high_cutoff], btype='band')\n",
    "\n",
    "# Apply the filter to each EEG channel\n",
    "filtered_eeg = np.zeros_like(eeg_data)\n",
    "for i in range(eeg_data.shape[0]):\n",
    "    filtered_eeg[i, :] = sig.filtfilt(b, a, eeg_data[i, :])\n",
    "\n",
    "# Keep original data for comparison in plots\n",
    "original_eeg = eeg_data.copy()\n",
    "eeg_data = filtered_eeg\n",
    "\n",
    "# Simple plot of the filtered eeg_data over the original eeg_data\n",
    "fig, axs = plt.subplots(2, 1)\n",
    "axs[0].plot(original_eeg[0, :fs_eeg*epoch_length*30])\n",
    "axs[0].plot(filtered_eeg[0, :fs_eeg*epoch_length*30], 'r-')\n",
    "axs[1].plot(original_eeg[1, :fs_eeg*epoch_length*30])\n",
    "axs[1].plot(filtered_eeg[1, :fs_eeg*epoch_length*30], 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICA (Independent Component Analysis) for EEG artifact removal:\n",
    "\n",
    "### 1. Data Preparation\n",
    "- The code combines multiple biosignals into a single matrix:\n",
    "  - 2 EEG channels (already filtered)\n",
    "  - 2 EOG (electrooculogram) channels for eye movements\n",
    "  - 1 EMG (electromyogram) channel for muscle activity\n",
    "  - 1 ECG (electrocardiogram) channel for heart activity\n",
    "- The data is transposed to match FastICA's expected format `[n_samples, n_features]`\n",
    "\n",
    "### 2. ICA Application\n",
    "- FastICA is applied to extract 6 independent components\n",
    "- Two key matrices are produced:\n",
    "  - `S`: The independent components (source signals)\n",
    "  - `A`: The mixing matrix (how components combine to form original signals)\n",
    "\n",
    "### 3. Visualization of Components\n",
    "- Plots all 6 ICA components for visual inspection\n",
    "- Shows 5 epochs of data for each component\n",
    "- Creates a mixing matrix visualization showing how each original channel contributes to the ICA components\n",
    "- Both plots are saved as PNG files\n",
    "\n",
    "### 4. Artifact Component Identification\n",
    "- Calculates correlations between:\n",
    "  - Each ICA component\n",
    "  - Each artifact channel (EOG, EMG, ECG)\n",
    "- Creates a correlation matrix visualization\n",
    "- Uses a correlation threshold (0.6) to identify components related to artifacts\n",
    "- Separately tracks components related to:\n",
    "  - Eye movements (EOG)\n",
    "  - Muscle activity (EMG)\n",
    "  - Heart activity (ECG)\n",
    "- Prints which components are associated with each type of artifact\n",
    "\n",
    "### 5. Signal Reconstruction\n",
    "- Creates cleaned signal by:\n",
    "  1. Making a copy of the ICA components\n",
    "  2. Zeroing out components identified as artifacts\n",
    "  3. Reconstructing the signal using the mixing matrix\n",
    "- Extracts only the cleaned EEG channels from the reconstruction\n",
    "\n",
    "### 6. Final Visualization\n",
    "- Creates comparison plots showing:\n",
    "  - Original filtered EEG\n",
    "  - ICA-cleaned EEG\n",
    "- Shows 5 epochs of data for each channel\n",
    "- Saves the comparison plot as a PNG file\n",
    "\n",
    "This is a standard artifact removal pipeline for EEG data using ICA. The process identifies components that represent artifacts (eye movements, muscle activity, heart beats) and removes them while preserving the underlying brain activity signals.\n",
    "\n",
    "The correlation threshold (0.6) is a tunable parameter - lower values will remove more components but might lose some genuine brain signals, while higher values are more conservative but might leave some artifacts in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ICA to the filtered eeg_data\n",
    "# Combine all channels for ICA\n",
    "all_channels = np.vstack([\n",
    "    eeg_data,      # 2 EEG channels (already filtered)\n",
    "    eog_data,      # 2 EOG channels \n",
    "    emg_data,      # 1 EMG channel\n",
    "    ecg_data       # 1 ECG channel\n",
    "])\n",
    "\n",
    "# Transpose for ICA (ICA expects [n_samples, n_features])\n",
    "X = all_channels.T\n",
    "\n",
    "# Apply FastICA to extract 6 components\n",
    "ica = FastICA(n_components=6, random_state=42)\n",
    "S = ica.fit_transform(X)  # S contains the independent components\n",
    "A = ica.mixing_  # A is the mixing matrix\n",
    "\n",
    "# Get the independent components (transposed back to [n_components, n_samples])\n",
    "components = S.T\n",
    "\n",
    "# Plot the 6 ICA components\n",
    "plt.figure(figsize=(12, 10))\n",
    "sample_length = fs_eeg * epoch_length * 5  # Show 5 epochs\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(6, 1, i+1)\n",
    "    plt.plot(components[i, :sample_length])\n",
    "    plt.title(f'ICA Component {i+1}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.savefig(get_output_path('ica_components.png'))\n",
    "plt.show()\n",
    "\n",
    "# Plot contribution of each original channel to each ICA component (mixing matrix)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(np.abs(A), aspect='auto', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.xlabel('ICA Components')\n",
    "plt.ylabel('Channels (EEG1, EEG2, EOGL, EOGR, EMG, ECG)')\n",
    "plt.title('Mixing Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig(get_output_path('mixing_matrix.png'))\n",
    "plt.show()\n",
    "\n",
    "# Filter out components that are correlated with the EOG, EMG and ECG channels\n",
    "# Find the components that are most correlated with the EOG, EMG and ECG channels\n",
    "\n",
    "# Calculate correlation between each component and each artifact channel\n",
    "artifact_channels = np.vstack([eog_data, emg_data, ecg_data])\n",
    "correlation_matrix = np.zeros((artifact_channels.shape[0], components.shape[0]))\n",
    "\n",
    "for i in range(artifact_channels.shape[0]):\n",
    "    for j in range(components.shape[0]):\n",
    "        correlation = np.abs(np.corrcoef(artifact_channels[i, :], components[j, :])[0, 1])\n",
    "        correlation_matrix[i, j] = correlation\n",
    "\n",
    "# Plot correlation matrix\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(correlation_matrix, aspect='auto', interpolation='nearest', cmap='viridis')\n",
    "plt.colorbar(label='Correlation Coefficient')\n",
    "plt.xlabel('ICA Components')\n",
    "plt.ylabel('Artifact Channels (EOGL, EOGR, EMG, ECG)')\n",
    "plt.title('Correlation between ICA Components and Artifact Channels')\n",
    "channel_names = ['EOGL', 'EOGR', 'EMG', 'ECG']\n",
    "plt.yticks(np.arange(len(channel_names)), channel_names)\n",
    "plt.xticks(np.arange(components.shape[0]), [f'Comp {i+1}' for i in range(components.shape[0])])\n",
    "plt.tight_layout()\n",
    "plt.savefig(get_output_path('correlation_matrix.png'))\n",
    "plt.show()\n",
    "\n",
    "# Identify components to remove (high correlation with artifact channels)\n",
    "# Correlation threshold for component removal - adjust based on data quality\n",
    "# Lower values (e.g., 0.2) will remove more components but may lose useful signal\n",
    "# Higher values (e.g., 0.4) will be more conservative but may retain more artifacts\n",
    "correlation_threshold = 0.6  # Adjust this value based on your dataset\n",
    "\n",
    "# Create separate sets for different artifact types\n",
    "eog_components = set()\n",
    "emg_components = set()\n",
    "ecg_components = set()\n",
    "components_to_remove = set()\n",
    "\n",
    "for i in range(correlation_matrix.shape[0]):\n",
    "    # Find components with correlation above threshold\n",
    "    high_corr_idx = np.where(correlation_matrix[i, :] > correlation_threshold)[0]\n",
    "    if len(high_corr_idx) > 0:\n",
    "        if i < 2:  # EOG channels (first two artifact channels)\n",
    "            print(f\"Channel {channel_names[i]} is highly correlated with components: {[idx+1 for idx in high_corr_idx]}\")\n",
    "            eog_components.update(high_corr_idx)\n",
    "        elif i == 2:  # EMG channel\n",
    "            print(f\"Channel {channel_names[i]} is highly correlated with components: {[idx+1 for idx in high_corr_idx]}\")\n",
    "            emg_components.update(high_corr_idx)\n",
    "        elif i == 3:  # ECG channel\n",
    "            print(f\"Channel {channel_names[i]} is highly correlated with components: {[idx+1 for idx in high_corr_idx]}\")\n",
    "            ecg_components.update(high_corr_idx)\n",
    "\n",
    "# Add all identified components to the removal set\n",
    "# You can comment out any line below to keep certain artifact types if needed\n",
    "components_to_remove.update(eog_components)  # Eye movement artifacts\n",
    "components_to_remove.update(emg_components)  # Muscle artifacts\n",
    "components_to_remove.update(ecg_components)  # Heart artifacts\n",
    "\n",
    "components_to_remove = list(components_to_remove)\n",
    "components_to_keep = [i for i in range(components.shape[0]) if i not in components_to_remove]\n",
    "\n",
    "print(f\"EOG-related components: {[idx+1 for idx in eog_components]}\")\n",
    "print(f\"EMG-related components: {[idx+1 for idx in emg_components]}\")\n",
    "print(f\"ECG-related components: {[idx+1 for idx in ecg_components]}\")\n",
    "print(f\"Components to remove: {[idx+1 for idx in components_to_remove]}\")\n",
    "print(f\"Components to keep: {[idx+1 for idx in components_to_keep]}\")\n",
    "\n",
    "# Reconstruct cleaned signal\n",
    "# Zero out the artifact components and reconstruct\n",
    "S_clean = S.copy()\n",
    "for idx in components_to_remove:\n",
    "    S_clean[:, idx] = 0\n",
    "\n",
    "X_clean = np.dot(S_clean, ica.mixing_.T) + ica.mean_\n",
    "\n",
    "# Reshape back to original format [channels, samples]\n",
    "cleaned_channels = X_clean.T\n",
    "cleaned_eeg = cleaned_channels[:2, :]  # Extract only EEG channels\n",
    "\n",
    "# Plot original filtered EEG vs cleaned EEG\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i in range(eeg_data.shape[0]):\n",
    "    plt.subplot(2, 1, i+1)\n",
    "    plt.plot(eeg_data[i, :fs_eeg*epoch_length*5], 'b-', label='Filtered')\n",
    "    plt.plot(cleaned_eeg[i, :fs_eeg*epoch_length*5], 'r-', label='ICA Cleaned')\n",
    "    plt.title(f'EEG Channel {i+1}: Filtered vs ICA Cleaned')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(get_output_path('cleaned_eeg.png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# perform a DWT on the cleaned eeg_data to extract frequency bands\n",
    "\n",
    "# Define wavelet and decomposition level\n",
    "wavelet = 'db4'\n",
    "level = 6  # Decomposition level\n",
    "\n",
    "# Frequency bands for DWT decomposition with fs=125Hz\n",
    "# Note: These are approximate ranges due to the nature of wavelet transforms\n",
    "# Actual passbands are not as sharp as in FIR/IIR filters\n",
    "#\n",
    "# At sampling rate of 125Hz:\n",
    "# - Nyquist frequency: 62.5 Hz\n",
    "# - DWT Level 1 detail coefficients (cD1): ~31.25-62.5 Hz (gamma)\n",
    "# - DWT Level 2 detail coefficients (cD2): ~15.625-31.25 Hz (beta)\n",
    "# - DWT Level 3 detail coefficients (cD3): ~7.8125-15.625 Hz (alpha/spindles)\n",
    "# - DWT Level 4 detail coefficients (cD4): ~3.90625-7.8125 Hz (theta)\n",
    "# - DWT Level 5 detail coefficients (cD5): ~1.953125-3.90625 Hz (delta)\n",
    "# - DWT Level 6 detail coefficients (cD6): ~0.9765625-1.953125 Hz (slow delta)\n",
    "# - DWT Level 6 approximation coefficients (cA6): ~0-0.9765625 Hz (very slow oscillations)\n",
    "\n",
    "# Perform DWT on cleaned EEG data (using channel 1)\n",
    "coeffs = pywt.wavedec(cleaned_eeg[0, :], wavelet, level=level)\n",
    "\n",
    "# Extract coefficients (1 approximation + 6 details)\n",
    "cA6, cD6, cD5, cD4, cD3, cD2, cD1 = coeffs\n",
    "\n",
    "# Create zero arrays with the same shape as the coefficients\n",
    "zero_cA6 = np.zeros_like(cA6)\n",
    "zero_cD6 = np.zeros_like(cD6)\n",
    "zero_cD5 = np.zeros_like(cD5)\n",
    "zero_cD4 = np.zeros_like(cD4)\n",
    "zero_cD3 = np.zeros_like(cD3)\n",
    "zero_cD2 = np.zeros_like(cD2)\n",
    "zero_cD1 = np.zeros_like(cD1)\n",
    "\n",
    "# Reconstruct each frequency band\n",
    "# For slow delta band (0-1 Hz): Use cA6\n",
    "coeffs_slow_delta = [cA6, zero_cD6, zero_cD5, zero_cD4, zero_cD3, zero_cD2, zero_cD1]\n",
    "slow_delta_band = pywt.waverec(coeffs_slow_delta, wavelet)[:len(cleaned_eeg[0, :])]\n",
    "\n",
    "# For delta band (1-4 Hz): Use cD6 (1-2 Hz) and cD5 (2-4 Hz)\n",
    "coeffs_delta = [zero_cA6, cD6, cD5, zero_cD4, zero_cD3, zero_cD2, zero_cD1]\n",
    "delta_band = pywt.waverec(coeffs_delta, wavelet)[:len(cleaned_eeg[0, :])]\n",
    "\n",
    "# For theta band (4-8 Hz): Use cD4\n",
    "coeffs_theta = [zero_cA6, zero_cD6, zero_cD5, cD4, zero_cD3, zero_cD2, zero_cD1]\n",
    "theta_band = pywt.waverec(coeffs_theta, wavelet)[:len(cleaned_eeg[0, :])]\n",
    "\n",
    "# For alpha/spindle band (8-16 Hz): Use cD3\n",
    "coeffs_alpha = [zero_cA6, zero_cD6, zero_cD5, zero_cD4, cD3, zero_cD2, zero_cD1]\n",
    "alpha_spindle_band = pywt.waverec(coeffs_alpha, wavelet)[:len(cleaned_eeg[0, :])]\n",
    "\n",
    "# For beta band (16-32 Hz): Use cD2\n",
    "coeffs_beta = [zero_cA6, zero_cD6, zero_cD5, zero_cD4, zero_cD3, cD2, zero_cD1]\n",
    "beta_band = pywt.waverec(coeffs_beta, wavelet)[:len(cleaned_eeg[0, :])]\n",
    "\n",
    "# For gamma band (32-62.5 Hz): Use cD1\n",
    "coeffs_gamma = [zero_cA6, zero_cD6, zero_cD5, zero_cD4, zero_cD3, zero_cD2, cD1]\n",
    "gamma_band = pywt.waverec(coeffs_gamma, wavelet)[:len(cleaned_eeg[0, :])]\n",
    "\n",
    "# Plot different frequency bands\n",
    "band_names = ['Slow Delta (0-1 Hz)', 'Delta (1-4 Hz)', 'Theta (4-8 Hz)', 'Alpha+Spindles (8-16 Hz)', 'Beta (16-32 Hz)', 'Gamma (32-62.5 Hz)']\n",
    "bands = [slow_delta_band, delta_band, theta_band, alpha_spindle_band, beta_band, gamma_band]\n",
    "\n",
    "plt.figure(figsize=(15, 14))\n",
    "\n",
    "# Plot original signal\n",
    "plt.subplot(7, 1, 1)\n",
    "plt.plot(cleaned_eeg[0, :fs_eeg*epoch_length*3])\n",
    "plt.title('Cleaned EEG (Channel 1)')\n",
    "\n",
    "# Plot each frequency band\n",
    "for i, (band, name) in enumerate(zip(bands, band_names)):\n",
    "    plt.subplot(7, 1, i+2)\n",
    "    plt.plot(band[:fs_eeg*epoch_length*3])\n",
    "    plt.title(name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(get_output_path('eeg_bands.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sleep spindle detection in EEG data:\n",
    "\n",
    "### 1. Sleep Spindle Background\n",
    "Sleep spindles are distinct patterns in EEG signals that:\n",
    "- Occur in the 10-15 Hz frequency range\n",
    "- Typically last between 0.5-2 seconds\n",
    "- Are important markers of stage 2 sleep\n",
    "\n",
    "### 2. Detection Parameters Setup\n",
    "The code establishes several key parameters:\n",
    "- Uses pre-filtered data in the alpha/spindle band (10-15 Hz)\n",
    "- Sets threshold factors:\n",
    "  - Lower threshold factor: 1.0\n",
    "  - Upper threshold factor: 5.0\n",
    "- Defines duration constraints:\n",
    "  - Minimum duration: 0.4 seconds\n",
    "  - Maximum duration: 2.0 seconds\n",
    "\n",
    "### 3. Signal Processing Steps\n",
    "1. **Envelope Calculation**\n",
    "   - Uses Hilbert transform to compute the signal envelope\n",
    "   - The envelope represents the amplitude modulation of the signal\n",
    "\n",
    "2. **Threshold Calculation**\n",
    "   - Computes mean of the envelope\n",
    "   - Sets two thresholds:\n",
    "     - Lower threshold = mean × 1.0\n",
    "     - Upper threshold = mean × 5.0\n",
    "\n",
    "3. **Spindle Detection Algorithm**\n",
    "   - Creates a mask where signal is between thresholds\n",
    "   - Scans through the mask to find continuous segments\n",
    "   - Validates segments based on duration criteria\n",
    "   - Stores valid spindles as (start, end, duration) tuples\n",
    "\n",
    "### 4. Visualization (Three-Panel Plot)\n",
    "1. **Top Panel**\n",
    "   - Shows the filtered signal in the spindle band (10-15 Hz)\n",
    "   - Displays 30 seconds of data\n",
    "\n",
    "2. **Middle Panel**\n",
    "   - Shows the signal envelope\n",
    "   - Displays both upper and lower thresholds as dashed lines\n",
    "\n",
    "3. **Bottom Panel**\n",
    "   - Shows the original filtered signal\n",
    "   - Highlights detected spindles with yellow transparent overlays\n",
    "   - Includes time axis in seconds\n",
    "\n",
    "### 5. Results Output\n",
    "- Prints total number of detected spindles\n",
    "- For the first 10 spindles, displays:\n",
    "  - Start time (in seconds)\n",
    "  - Duration (in seconds)\n",
    "\n",
    "The method looks for sustained periods where the signal envelope falls within specific amplitude bounds and meets duration criteria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect sleep spindles (10-15 Hz in alpha_spindle_band)\n",
    "# Spindles typically have frequencies between 10-15 Hz and last 0.5-2 seconds\n",
    "\n",
    "# Use the alpha_spindle_band directly instead of applying additional filtering\n",
    "spindle_band = alpha_spindle_band\n",
    "\n",
    "# Calculate the signal envelope\n",
    "spindle_envelope = np.abs(sig.hilbert(spindle_band))\n",
    "\n",
    "threshold_factor_lower = 1.0\n",
    "threshold_factor_upper = 5.0\n",
    "min_duration = 0.4\n",
    "max_duration = 2.0\n",
    "\n",
    "# Define a threshold based on the mean envelope\n",
    "mean_envelope = np.mean(spindle_envelope)\n",
    "lower_threshold = threshold_factor_lower * mean_envelope\n",
    "upper_threshold = threshold_factor_upper * mean_envelope\n",
    "\n",
    "# 3) Create a mask that is True only where envelope is between lower and upper thresholds\n",
    "valid_mask = (spindle_envelope >= lower_threshold) & (spindle_envelope <= upper_threshold)\n",
    "\n",
    "# 4) Scan the mask to detect contiguous segments that qualify as spindles\n",
    "spindles = []\n",
    "i = 0\n",
    "n_samples = len(spindle_band)\n",
    "\n",
    "while i < n_samples:\n",
    "    if valid_mask[i]:\n",
    "        start = i\n",
    "        # Move forward while we remain within valid_mask == True\n",
    "        while i < n_samples and valid_mask[i]:\n",
    "            i += 1\n",
    "        end = i\n",
    "        duration = (end - start) / fs_eeg\n",
    "\n",
    "        # 5) Check duration constraints\n",
    "        if min_duration <= duration <= max_duration:\n",
    "            spindles.append((start, end, duration))\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Create time array for x-axis\n",
    "t = np.arange(len(spindle_band)) / fs_eeg\n",
    "\n",
    "# Plot a subset of the data for better visualization\n",
    "display_duration = 30  # seconds\n",
    "display_samples = min(display_duration * fs_eeg, len(spindle_band))\n",
    "\n",
    "# Plot original signal\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(t[:display_samples], spindle_band[:display_samples])\n",
    "plt.title('Spindle Band (10-15 Hz)')\n",
    "\n",
    "# Plot spindle band and envelope\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(t[:display_samples], spindle_envelope[:display_samples])\n",
    "plt.axhline(y=lower_threshold, color='r', linestyle='--', label=f'Lower Threshold ({lower_threshold:.2f})')\n",
    "plt.axhline(y=upper_threshold, color='g', linestyle='--', label=f'Upper Threshold ({upper_threshold:.2f})')\n",
    "plt.legend()\n",
    "plt.title('Spindle Envelope with Thresholds')\n",
    "\n",
    "# Highlight detected spindles\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(t[:display_samples], spindle_band[:display_samples])\n",
    "\n",
    "# Highlight detected spindle intervals\n",
    "for start, end, duration in spindles:\n",
    "    if start < display_samples:  # Only highlight spindles in the displayed time window\n",
    "        plt.axvspan(t[start], t[end], color='yellow', alpha=0.3)\n",
    "\n",
    "plt.title(f'Detected Spindles (Found {len(spindles)} spindles)')\n",
    "plt.xlabel('Time (s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print information about detected spindles\n",
    "print(f\"Detected {len(spindles)} sleep spindles\")\n",
    "for i, (start, end, duration) in enumerate(spindles[:10]):  # Show details for first 10 spindles\n",
    "    start_time = start / fs_eeg\n",
    "    print(f\"Spindle {i+1}: Start: {start_time:.2f}s, Duration: {duration:.2f}s\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the total psd of each band for each epoch\n",
    "\n",
    "# Number of epochs in the entire recording\n",
    "num_epochs = len(cleaned_eeg[0, :]) // (fs_eeg * epoch_length)\n",
    "print(f\"Total number of epochs: {num_epochs}\")\n",
    "\n",
    "# Initialize arrays to store PSD values for each band and epoch\n",
    "band_names_short = ['slow_delta', 'delta', 'theta', 'alpha', 'beta', 'gamma']\n",
    "all_bands = [slow_delta_band, delta_band, theta_band, alpha_spindle_band, beta_band, gamma_band]\n",
    "\n",
    "# Create dictionary to store band powers by epoch\n",
    "band_powers = {band_name: [] for band_name in band_names_short}\n",
    "epoch_timestamps = []  # Store timestamp for each epoch (in seconds)\n",
    "\n",
    "# Calculate frequency bands' power for each epoch\n",
    "for epoch_idx in range(num_epochs):\n",
    "    # Calculate epoch start and end indices\n",
    "    start_idx = epoch_idx * fs_eeg * epoch_length\n",
    "    end_idx = (epoch_idx + 1) * fs_eeg * epoch_length\n",
    "    \n",
    "    # Store epoch timestamp (middle of epoch in seconds)\n",
    "    epoch_time = (start_idx + (fs_eeg * epoch_length) / 2) / fs_eeg\n",
    "    epoch_timestamps.append(epoch_time)\n",
    "    \n",
    "    # Calculate band powers for this epoch\n",
    "    for band_idx, (band, band_name) in enumerate(zip(all_bands, band_names_short)):\n",
    "        # Get the epoch segment for this band\n",
    "        epoch_segment = band[start_idx:end_idx]\n",
    "        \n",
    "        # Calculate PSD using Welch's method\n",
    "        f, Pxx = sig.welch(epoch_segment, fs=fs_eeg, nperseg=min(len(epoch_segment), fs_eeg*4))\n",
    "        \n",
    "        # Calculate total power in band (area under PSD curve)\n",
    "        total_power = np.sum(Pxx)\n",
    "        band_powers[band_name].append(total_power)\n",
    "\n",
    "# Convert lists to numpy arrays for easier manipulation\n",
    "for band_name in band_names_short:\n",
    "    band_powers[band_name] = np.array(band_powers[band_name])\n",
    "epoch_timestamps = np.array(epoch_timestamps)\n",
    "\n",
    "# Plot band powers across epochs\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "for i, band_name in enumerate(band_names_short):\n",
    "    plt.subplot(len(band_names_short), 1, i+1)\n",
    "    plt.plot(epoch_timestamps / 3600, band_powers[band_name])  # Convert to hours for x-axis\n",
    "    plt.title(f'{band_name.capitalize()} Band Power')\n",
    "    plt.xlabel('Time (hours)')\n",
    "    plt.ylabel('Power')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(get_output_path('band_powers_by_epoch.png'))\n",
    "plt.show()\n",
    "\n",
    "# Create a stacked plot showing relative band powers\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Calculate relative powers (normalized for each epoch)\n",
    "rel_powers = np.zeros((len(band_names_short), num_epochs))\n",
    "for i, band_name in enumerate(band_names_short):\n",
    "    rel_powers[i, :] = band_powers[band_name]\n",
    "\n",
    "# Normalize so total power at each epoch = 1\n",
    "epoch_totals = np.sum(rel_powers, axis=0)\n",
    "for i in range(len(band_names_short)):\n",
    "    rel_powers[i, :] = rel_powers[i, :] / epoch_totals\n",
    "\n",
    "# Save the band powers to a .mat file for further analysis\n",
    "band_powers_output = {\n",
    "    'timestamps': epoch_timestamps,\n",
    "    'slow_delta': band_powers['slow_delta'],\n",
    "    'delta': band_powers['delta'],\n",
    "    'theta': band_powers['theta'],\n",
    "    'alpha': band_powers['alpha'],\n",
    "    'beta': band_powers['beta'],\n",
    "    'gamma': band_powers['gamma']\n",
    "}\n",
    "\n",
    "sio.savemat(get_output_path('band_powers.mat'), band_powers_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sleep stages from the .mat file\n",
    "sleep_stages = sio.loadmat('hypnogram.mat')['stages']\n",
    "print(f\"Shape of sleep stages: {sleep_stages.shape}\")\n",
    "\n",
    "# Convert to 1D array if needed\n",
    "if sleep_stages.ndim > 1:\n",
    "    sleep_stages = sleep_stages.flatten()\n",
    "\n",
    "# Resample sleep stages to match the number of epochs in our band powers\n",
    "# Since sleep_stages have many more data points, we'll take one value per chunk\n",
    "if len(sleep_stages) != num_epochs:\n",
    "    print(f\"Resampling sleep stages from {len(sleep_stages)} to {num_epochs}\")\n",
    "    resample_factor = len(sleep_stages) / num_epochs\n",
    "    resampled_sleep_stages = np.zeros(num_epochs)\n",
    "    \n",
    "    # Resampling method options:\n",
    "    # 1. 'mode' - Most frequent value (default)\n",
    "    # 2. 'first' - First value in chunk\n",
    "    # 3. 'middle' - Middle value in chunk\n",
    "    # 4. 'last' - Last value in chunk\n",
    "    resample_method = 'mode'  # Select the method here\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        # Calculate indices for this chunk\n",
    "        start_idx = int(i * resample_factor)\n",
    "        end_idx = int((i + 1) * resample_factor)\n",
    "        \n",
    "        # Ensure indices are within bounds\n",
    "        start_idx = max(0, min(start_idx, len(sleep_stages)-1))\n",
    "        end_idx = max(start_idx+1, min(end_idx, len(sleep_stages)))\n",
    "        \n",
    "        # Extract chunk\n",
    "        chunk = sleep_stages[start_idx:end_idx]\n",
    "        \n",
    "        if len(chunk) > 0:\n",
    "            if resample_method == 'mode':\n",
    "                # Use most frequent value (mode)\n",
    "                values, counts = np.unique(chunk, return_counts=True)\n",
    "                resampled_sleep_stages[i] = values[np.argmax(counts)]\n",
    "            elif resample_method == 'first':\n",
    "                # Use first value in chunk\n",
    "                resampled_sleep_stages[i] = chunk[0]\n",
    "            elif resample_method == 'middle':\n",
    "                # Use middle value in chunk\n",
    "                resampled_sleep_stages[i] = chunk[len(chunk)//2]\n",
    "            elif resample_method == 'last':\n",
    "                # Use last value in chunk\n",
    "                resampled_sleep_stages[i] = chunk[-1]\n",
    "            else:\n",
    "                # Default to mode\n",
    "                values, counts = np.unique(chunk, return_counts=True)\n",
    "                resampled_sleep_stages[i] = values[np.argmax(counts)]\n",
    "    \n",
    "    sleep_stages = resampled_sleep_stages\n",
    "\n",
    "# Plot sleep stages. Each column corresponds to a 30 second epoch\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.step(epoch_timestamps / 3600, sleep_stages, where='post')\n",
    "plt.yticks([0, 1, 2, 3, 4, 5], ['REM', '', 'N3', 'N2', 'N1', 'Wake'])\n",
    "plt.xlabel('Time (hours)')\n",
    "plt.ylabel('Sleep Stage')\n",
    "plt.title('Hypnogram')\n",
    "plt.grid(True, axis='y')\n",
    "plt.ylim(-0.5, 5.5)\n",
    "plt.savefig(get_output_path('hypnogram.png'))\n",
    "plt.show()\n",
    "\n",
    "# Create a stacked plot showing relative band powers with sleep stages\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12), gridspec_kw={'height_ratios': [1, 3]})\n",
    "\n",
    "# Plot hypnogram on top subplot\n",
    "ax1.step(epoch_timestamps / 3600, sleep_stages, where='post', color='black', linewidth=1.5)\n",
    "ax1.set_yticks([0, 1, 2, 3, 4, 5])\n",
    "ax1.set_yticklabels(['REM', '', 'N3', 'N2', 'N1', 'Wake'])\n",
    "ax1.set_ylabel('Sleep Stage')\n",
    "ax1.set_title('Sleep Architecture and EEG Band Powers')\n",
    "ax1.grid(True, axis='y')\n",
    "ax1.set_ylim(-0.5, 5.5)\n",
    "\n",
    "# Plot stacked relative powers on bottom subplot\n",
    "colors = ['darkblue', 'royalblue', 'green', 'orange', 'red', 'purple']\n",
    "ax2.stackplot(epoch_timestamps / 3600, rel_powers, labels=band_names_short, colors=colors)\n",
    "ax2.set_xlabel('Time (hours)')\n",
    "ax2.set_ylabel('Relative Power')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Ensure the x-axes are aligned\n",
    "ax1.set_xlim(ax2.get_xlim())\n",
    "ax1.tick_params(labelbottom=False)  # Hide x-ticks on top plot\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(get_output_path('sleep_stages_with_band_powers.png'))\n",
    "plt.show()\n",
    "\n",
    "# Calculate average band powers for each sleep stage\n",
    "unique_stages = np.unique(sleep_stages)\n",
    "avg_powers_by_stage = {stage: {band: [] for band in band_names_short} for stage in unique_stages}\n",
    "\n",
    "# Ensure sleep_stages array matches the number of epochs we have power data for\n",
    "min_length = min(len(sleep_stages), num_epochs)\n",
    "\n",
    "for stage in unique_stages:\n",
    "    stage_indices = np.where(sleep_stages[:min_length] == stage)[0]\n",
    "    for band_name in band_names_short:\n",
    "        if len(stage_indices) > 0:\n",
    "            avg_powers_by_stage[stage][band_name] = np.mean(band_powers[band_name][stage_indices])\n",
    "\n",
    "# Plot average band powers by sleep stage\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Prepare data for grouped bar chart\n",
    "stage_labels = ['REM', '', 'N3', 'N2', 'N1', 'Wake']\n",
    "stage_mapping = {0: 'REM', 1: '', 2: 'N3', 3: 'N2', 4: 'N1', 5: 'Wake'}\n",
    "x = np.arange(len(band_names_short))\n",
    "bar_width = 0.15\n",
    "\n",
    "for i, stage in enumerate(unique_stages):\n",
    "    if stage in range(len(stage_labels)):  # Ensure the stage is valid\n",
    "        stage_data = [avg_powers_by_stage[stage][band] for band in band_names_short]\n",
    "        offset = i - len(unique_stages)/2 + 0.5\n",
    "        plt.bar(x + offset*bar_width, stage_data, bar_width, label=stage_mapping.get(stage, f'Stage {stage}'))\n",
    "\n",
    "plt.xlabel('Frequency Band')\n",
    "plt.ylabel('Average Power')\n",
    "plt.title('Average Band Powers by Sleep Stage')\n",
    "plt.xticks(x, [name.capitalize() for name in band_names_short])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(get_output_path('avg_power_by_sleep_stage.png'))\n",
    "plt.show()\n",
    "\n",
    "# Save the average powers by sleep stage to the mat file\n",
    "stage_powers = {}\n",
    "for stage in unique_stages:\n",
    "    if stage in range(len(stage_labels)):\n",
    "        stage_name = stage_mapping.get(stage, f'stage_{stage}')\n",
    "        stage_powers[stage_name] = {band: avg_powers_by_stage[stage][band] for band in band_names_short}\n",
    "\n",
    "# Add to existing output dictionary\n",
    "band_powers_output['stage_powers'] = stage_powers\n",
    "\n",
    "# Save again with updated information\n",
    "sio.savemat(get_output_path('band_powers.mat'), band_powers_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG feature extraction and analysis:\n",
    "\n",
    "1. Feature Organization Structure\n",
    "The code organizes features into several categories:\n",
    "- Absolute power (raw power in each frequency band)\n",
    "- Relative power (normalized power per band)\n",
    "- Power ratios (relationships between bands)\n",
    "- Spectral features (complexity measures)\n",
    "- Time-domain features (statistical measures)\n",
    "\n",
    "## 2. Feature Types and Calculations\n",
    "\n",
    "### A. Power-Based Features\n",
    "1. **Absolute Power**\n",
    "   - Uses pre-calculated band powers for each frequency range\n",
    "   - Includes: slow delta, delta, theta, alpha, beta, gamma\n",
    "\n",
    "2. **Relative Power**\n",
    "   - Normalizes each band's power by total power\n",
    "   - Helps account for overall amplitude variations\n",
    "\n",
    "3. **Power Ratios**\n",
    "   - Calculates important band relationships:\n",
    "     - Slow delta/delta\n",
    "     - Delta/theta\n",
    "     - Theta/alpha\n",
    "     - Alpha/beta\n",
    "     - Delta/beta\n",
    "     - Theta/beta\n",
    "     - Slow delta/gamma\n",
    "     - Delta/gamma\n",
    "\n",
    "### B. Spectral Complexity Features\n",
    "1. **Spectral Entropy**\n",
    "   - Measures signal randomness/complexity\n",
    "   - Higher values indicate more random signals\n",
    "\n",
    "2. **Spectral Edge Frequencies**\n",
    "   - SEF50 (median frequency)\n",
    "   - SEF95 (95th percentile frequency)\n",
    "\n",
    "### C. Time Domain Features\n",
    "1. **Statistical Measures**\n",
    "   - Variance\n",
    "   - Skewness\n",
    "   - Kurtosis\n",
    "   - Zero-crossing rate\n",
    "\n",
    "2. **Hjorth Parameters**\n",
    "   - Activity (signal variance)\n",
    "   - Mobility (mean frequency)\n",
    "   - Complexity (change in frequency)\n",
    "\n",
    "3. **Spindle Metrics**\n",
    "   - Spindle density (spindles per minute)\n",
    "\n",
    "## 3. Visualization and Analysis\n",
    "\n",
    "### A. Sleep Stage and Spindle Plot\n",
    "- Two-panel plot showing:\n",
    "  - Hypnogram (sleep stages over time)\n",
    "  - Spindle occurrences by sleep stage\n",
    "\n",
    "### B. Feature Analysis Plots\n",
    "1. **Delta vs. Theta Power**\n",
    "   - Scatter plot colored by sleep stage\n",
    "\n",
    "2. **Spectral Entropy**\n",
    "   - Box plot across sleep stages\n",
    "\n",
    "3. **Spindle Density**\n",
    "   - Bar chart of average density per stage\n",
    "\n",
    "4. **Delta/Beta Ratio**\n",
    "   - Box plot across sleep stages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "print(\"Extracting EEG features...\")\n",
    "\n",
    "# Initialize dictionaries to store all features\n",
    "features = {\n",
    "    'timestamps': epoch_timestamps,\n",
    "    'absolute_power': {},  # Store absolute power for each band\n",
    "    'relative_power': {},  # Store relative power (band power / total power)\n",
    "    'power_ratios': {},    # Store power ratios between bands\n",
    "    'spectral_features': {},  # Store spectral entropy and complexity measures\n",
    "    'time_domain': {}      # Store time-domain statistics\n",
    "}\n",
    "\n",
    "# 1. Absolute and Relative Powers\n",
    "total_power = np.zeros(num_epochs)\n",
    "for band_name in band_names_short:\n",
    "    # Store absolute power (already calculated)\n",
    "    features['absolute_power'][band_name] = band_powers[band_name]\n",
    "    # Add to total power for relative calculations\n",
    "    total_power += band_powers[band_name]\n",
    "\n",
    "# Calculate relative power for each band\n",
    "for band_name in band_names_short:\n",
    "    features['relative_power'][band_name] = band_powers[band_name] / total_power\n",
    "\n",
    "# 2. Power Ratios - Calculate ratios between bands\n",
    "ratio_pairs = [\n",
    "    ('slow_delta', 'delta'),    # Slow delta to delta ratio\n",
    "    ('delta', 'theta'),         # Delta to theta ratio\n",
    "    ('theta', 'alpha'),         # Theta to alpha ratio\n",
    "    ('alpha', 'beta'),          # Alpha to beta ratio\n",
    "    ('delta', 'beta'),          # Delta to beta ratio (often used in sleep studies)\n",
    "    ('theta', 'beta'),          # Theta to beta ratio (attention/concentration)\n",
    "    ('slow_delta', 'gamma'),    # Slow delta to gamma ratio\n",
    "    ('delta', 'gamma')          # Delta to gamma ratio\n",
    "]\n",
    "\n",
    "for band1, band2 in ratio_pairs:\n",
    "    ratio_name = f\"{band1}_to_{band2}\"\n",
    "    # Avoid division by zero with small epsilon\n",
    "    features['power_ratios'][ratio_name] = band_powers[band1] / (band_powers[band2] + 1e-10)\n",
    "\n",
    "# 3. Spectral Complexity Measures\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Calculate spectral entropy for each epoch\n",
    "spectral_entropy = np.zeros(num_epochs)\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    # Extract epoch data\n",
    "    start_idx = epoch_idx * fs_eeg * epoch_length\n",
    "    end_idx = (epoch_idx + 1) * fs_eeg * epoch_length\n",
    "    epoch_data = cleaned_eeg[0, start_idx:end_idx]\n",
    "    \n",
    "    # Calculate PSD\n",
    "    f, Pxx = sig.welch(epoch_data, fs=fs_eeg, nperseg=min(len(epoch_data), fs_eeg*4))\n",
    "    \n",
    "    # Normalize PSD to calculate entropy (like a probability distribution)\n",
    "    Pxx_norm = Pxx / np.sum(Pxx)\n",
    "    \n",
    "    # Calculate entropy (higher entropy = more complex/random signal)\n",
    "    spectral_entropy[epoch_idx] = entropy(Pxx_norm)\n",
    "\n",
    "features['spectral_features']['spectral_entropy'] = spectral_entropy\n",
    "\n",
    "# Spectral edge frequency (frequency below which X% of power resides)\n",
    "# 50% (median frequency) and 95% spectral edge\n",
    "spectral_edge_50 = np.zeros(num_epochs)\n",
    "spectral_edge_95 = np.zeros(num_epochs)\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    start_idx = epoch_idx * fs_eeg * epoch_length\n",
    "    end_idx = (epoch_idx + 1) * fs_eeg * epoch_length\n",
    "    epoch_data = cleaned_eeg[0, start_idx:end_idx]\n",
    "    \n",
    "    # Calculate PSD\n",
    "    f, Pxx = sig.welch(epoch_data, fs=fs_eeg, nperseg=min(len(epoch_data), fs_eeg*4))\n",
    "    \n",
    "    # Calculate cumulative sum of power\n",
    "    cumsum_pxx = np.cumsum(Pxx) / np.sum(Pxx)\n",
    "    \n",
    "    # Find frequencies at 50% and 95% power\n",
    "    idx_50 = np.argmax(cumsum_pxx >= 0.5)\n",
    "    idx_95 = np.argmax(cumsum_pxx >= 0.95)\n",
    "    \n",
    "    spectral_edge_50[epoch_idx] = f[idx_50]\n",
    "    spectral_edge_95[epoch_idx] = f[idx_95]\n",
    "\n",
    "features['spectral_features']['spectral_edge_50'] = spectral_edge_50\n",
    "features['spectral_features']['spectral_edge_95'] = spectral_edge_95\n",
    "\n",
    "# 4. Time Domain Features\n",
    "# Calculate variance, skewness, kurtosis, zero-crossings, and Hjorth parameters\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "variance = np.zeros(num_epochs)\n",
    "skewness = np.zeros(num_epochs)\n",
    "kurtosis_val = np.zeros(num_epochs)\n",
    "zero_crossings = np.zeros(num_epochs)\n",
    "hjorth_activity = np.zeros(num_epochs)     # variance of signal (first derivative)\n",
    "hjorth_mobility = np.zeros(num_epochs)     # sqrt(variance of first derivative / variance of signal)\n",
    "hjorth_complexity = np.zeros(num_epochs)   # ratio of mobility of first derivative to mobility of signal\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    start_idx = epoch_idx * fs_eeg * epoch_length\n",
    "    end_idx = (epoch_idx + 1) * fs_eeg * epoch_length\n",
    "    epoch_data = cleaned_eeg[0, start_idx:end_idx]\n",
    "    \n",
    "    # Basic statistics\n",
    "    variance[epoch_idx] = np.var(epoch_data)\n",
    "    skewness[epoch_idx] = skew(epoch_data)\n",
    "    kurtosis_val[epoch_idx] = kurtosis(epoch_data)\n",
    "    \n",
    "    # Zero crossings (sign changes)\n",
    "    zero_crossings[epoch_idx] = np.sum(np.diff(np.signbit(epoch_data)))\n",
    "    \n",
    "    # Hjorth parameters\n",
    "    # First derivative\n",
    "    derivative = np.diff(epoch_data)\n",
    "    # Second derivative\n",
    "    derivative2 = np.diff(derivative)\n",
    "    \n",
    "    # Activity = variance of signal\n",
    "    hjorth_activity[epoch_idx] = np.var(epoch_data)\n",
    "    \n",
    "    # Mobility = sqrt(variance of first derivative / variance of signal)\n",
    "    mobility_signal = np.sqrt(np.var(derivative) / (np.var(epoch_data) + 1e-10))\n",
    "    hjorth_mobility[epoch_idx] = mobility_signal\n",
    "    \n",
    "    # Complexity = mobility of first derivative / mobility of signal\n",
    "    mobility_derivative = np.sqrt(np.var(derivative2) / (np.var(derivative) + 1e-10))\n",
    "    hjorth_complexity[epoch_idx] = mobility_derivative / (mobility_signal + 1e-10)\n",
    "\n",
    "features['time_domain']['variance'] = variance\n",
    "features['time_domain']['skewness'] = skewness\n",
    "features['time_domain']['kurtosis'] = kurtosis_val\n",
    "features['time_domain']['zero_crossings'] = zero_crossings\n",
    "features['time_domain']['hjorth_activity'] = hjorth_activity\n",
    "features['time_domain']['hjorth_mobility'] = hjorth_mobility\n",
    "features['time_domain']['hjorth_complexity'] = hjorth_complexity\n",
    "\n",
    "# 5. Additionally for spindles - calculate spindle density (spindles per minute) for each epoch\n",
    "spindle_density = np.zeros(num_epochs)\n",
    "for idx, (start, end, duration) in enumerate(spindles):\n",
    "    # Find which epoch this spindle belongs to\n",
    "    epoch_idx = int(start / (fs_eeg * epoch_length))\n",
    "    if epoch_idx < num_epochs:\n",
    "        # Increment the spindle count for this epoch\n",
    "        spindle_density[epoch_idx] += 1\n",
    "\n",
    "# Convert to spindles per minute (30-second epochs = 2 epochs per minute)\n",
    "spindle_density = spindle_density / 0.5  # per minute\n",
    "features['time_domain']['spindle_density'] = spindle_density\n",
    "\n",
    "# Create a stacked bar plot to show spindles per epoch alongside sleep stages\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot sleep stages (hypnogram) on top subplot\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.step(epoch_timestamps / 3600, sleep_stages, where='post', color='black', linewidth=1.5)\n",
    "plt.yticks([0, 1, 2, 3, 4, 5], ['REM', '', 'N3', 'N2', 'N1', 'Wake'])\n",
    "plt.ylabel('Sleep Stage')\n",
    "plt.title('Sleep Stages and Spindle Occurrences')\n",
    "plt.grid(True, axis='y')\n",
    "plt.ylim(-0.5, 5.5)\n",
    "\n",
    "# Plot spindle count per epoch on bottom subplot\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Color-code by sleep stage\n",
    "colors = {0: 'purple', 2: 'darkblue', 3: 'royalblue', 4: 'lightblue', 5: 'gray'}\n",
    "stage_names = {0: 'REM', 2: 'N3', 3: 'N2', 4: 'N1', 5: 'Wake'}\n",
    "\n",
    "# Get unique stages for legend\n",
    "unique_stages = np.unique(sleep_stages)\n",
    "\n",
    "# Plot bars with different colors based on sleep stage\n",
    "for stage in unique_stages:\n",
    "    if stage in stage_names:  # Skip empty stage label (1)\n",
    "        mask = (sleep_stages == stage)\n",
    "        if np.any(mask):\n",
    "            stage_epochs = np.where(mask)[0]\n",
    "            plt.bar(epoch_timestamps[stage_epochs] / 3600, \n",
    "                    spindle_density[stage_epochs] * 0.5,  # Convert back to counts\n",
    "                    width=0.02,  # Adjust width for visibility\n",
    "                    color=colors.get(stage, 'black'),\n",
    "                    label=stage_names.get(stage, f'Stage {stage}'))\n",
    "\n",
    "plt.xlabel('Time (hours)')\n",
    "plt.ylabel('Spindle Count per Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Ensure the x-axes are aligned between the two subplots\n",
    "plt.subplot(2, 1, 1)\n",
    "xlim = plt.gca().get_xlim()\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.xlim(xlim)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(get_output_path('spindles_per_epoch.png'))\n",
    "plt.show()\n",
    "\n",
    "# Save all features to a mat file\n",
    "sio.savemat(get_output_path('eeg_features.mat'), features)\n",
    "\n",
    "# Save as CSV for easier analysis in other tools\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with all features\n",
    "feature_df = pd.DataFrame({\n",
    "    'timestamp': epoch_timestamps,\n",
    "    'sleep_stage': sleep_stages\n",
    "})\n",
    "\n",
    "# Add all calculated features to the DataFrame\n",
    "# Absolute Power\n",
    "for band in band_names_short:\n",
    "    feature_df[f'abs_power_{band}'] = features['absolute_power'][band]\n",
    "\n",
    "# Relative Power\n",
    "for band in band_names_short:\n",
    "    feature_df[f'rel_power_{band}'] = features['relative_power'][band]\n",
    "\n",
    "# Power Ratios\n",
    "for ratio_name in features['power_ratios']:\n",
    "    feature_df[f'ratio_{ratio_name}'] = features['power_ratios'][ratio_name]\n",
    "\n",
    "# Spectral Features\n",
    "for spec_feature in features['spectral_features']:\n",
    "    feature_df[f'spectral_{spec_feature}'] = features['spectral_features'][spec_feature]\n",
    "\n",
    "# Time Domain Features\n",
    "for time_feature in features['time_domain']:\n",
    "    feature_df[f'time_{time_feature}'] = features['time_domain'][time_feature]\n",
    "\n",
    "# Save to CSV\n",
    "feature_df.to_csv(get_output_path('eeg_features.csv'), index=False)\n",
    "\n",
    "print(f\"Feature extraction completed. Saved {len(feature_df.columns)-2} features for {num_epochs} epochs.\")\n",
    "\n",
    "# Plot some of the key features across sleep stages\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot relative powers by sleep stage\n",
    "plt.subplot(2, 2, 1)\n",
    "for stage in unique_stages:\n",
    "    if stage in stage_mapping and stage_mapping[stage]:  # Skip empty stage label\n",
    "        stage_indices = np.where(sleep_stages == stage)[0]\n",
    "        if len(stage_indices) > 0:\n",
    "            plt.scatter(\n",
    "                features['relative_power']['delta'][stage_indices], \n",
    "                features['relative_power']['theta'][stage_indices],\n",
    "                alpha=0.3, \n",
    "                label=stage_mapping[stage]\n",
    "            )\n",
    "            \n",
    "plt.xlabel('Relative Delta Power')\n",
    "plt.ylabel('Relative Theta Power')\n",
    "plt.title('Delta vs. Theta Relative Power by Sleep Stage')\n",
    "plt.legend()\n",
    "\n",
    "# Plot spectral entropy by sleep stage (boxplot)\n",
    "plt.subplot(2, 2, 2)\n",
    "stage_entropies = []\n",
    "stage_labels_present = []\n",
    "for stage in unique_stages:\n",
    "    if stage in stage_mapping and stage_mapping[stage]:  # Skip empty stage label\n",
    "        stage_indices = np.where(sleep_stages == stage)[0]\n",
    "        if len(stage_indices) > 0:\n",
    "            stage_entropies.append(features['spectral_features']['spectral_entropy'][stage_indices])\n",
    "            stage_labels_present.append(stage_mapping[stage])\n",
    "\n",
    "plt.boxplot(stage_entropies)\n",
    "plt.xticks(range(1, len(stage_labels_present) + 1), stage_labels_present)\n",
    "plt.ylabel('Spectral Entropy')\n",
    "plt.title('Spectral Entropy by Sleep Stage')\n",
    "\n",
    "# Plot spindle density by sleep stage (bar chart)\n",
    "plt.subplot(2, 2, 3)\n",
    "stage_densities = []\n",
    "for stage in unique_stages:\n",
    "    if stage in stage_mapping and stage_mapping[stage]:  # Skip empty stage label\n",
    "        stage_indices = np.where(sleep_stages == stage)[0]\n",
    "        if len(stage_indices) > 0:\n",
    "            avg_density = np.mean(features['time_domain']['spindle_density'][stage_indices])\n",
    "            stage_densities.append((stage_mapping[stage], avg_density))\n",
    "\n",
    "if stage_densities:  # Ensure we have data to plot\n",
    "    labels, values = zip(*sorted(stage_densities, key=lambda x: x[1], reverse=True))\n",
    "    plt.bar(labels, values)\n",
    "    plt.ylabel('Spindles per Minute')\n",
    "    plt.title('Average Spindle Density by Sleep Stage')\n",
    "\n",
    "# Plot Delta/Beta ratio by sleep stage (boxplot)\n",
    "plt.subplot(2, 2, 4)\n",
    "stage_ratios = []\n",
    "for stage in unique_stages:\n",
    "    if stage in stage_mapping and stage_mapping[stage]:  # Skip empty stage label\n",
    "        stage_indices = np.where(sleep_stages == stage)[0]\n",
    "        if len(stage_indices) > 0:\n",
    "            stage_ratios.append(features['power_ratios']['delta_to_beta'][stage_indices])\n",
    "\n",
    "if stage_ratios:  # Ensure we have data to plot\n",
    "    plt.boxplot(stage_ratios)\n",
    "    plt.xticks(range(1, len(stage_labels_present) + 1), stage_labels_present)\n",
    "    plt.ylabel('Delta/Beta Ratio')\n",
    "    plt.title('Delta/Beta Ratio by Sleep Stage')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(get_output_path('feature_analysis.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sleep stage classification pipeline:\n",
    "\n",
    "### 1. Data Preparation\n",
    "- Loads features from CSV file\n",
    "- Handles data cleaning:\n",
    "  - Removes NaN values\n",
    "  - Filters out invalid sleep stages\n",
    "  - Keeps valid stages: REM (0), N3 (2), N2 (3), N1 (4), Wake (5)\n",
    "- Splits data into features (X) and labels (y)\n",
    "- Uses LabelEncoder to convert sleep stages to consecutive integers\n",
    "\n",
    "### 2. Feature Selection and Processing\n",
    "1. **Initial Feature Importance Analysis**\n",
    "   - Uses preliminary Random Forest to rank features\n",
    "   - Visualizes top 20 feature importances\n",
    "   - Selects top 20 most important features\n",
    "\n",
    "2. **Feature Scaling**\n",
    "   - Applies StandardScaler to normalize features\n",
    "   - Transforms both training and validation sets\n",
    "\n",
    "### 3. Model Implementation\n",
    "Implements three different classifiers:\n",
    "\n",
    "#### A. Random Forest\n",
    "```python\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,  # Number of trees\n",
    "    max_depth=15,      # Maximum tree depth\n",
    "    random_state=42,   \n",
    "    n_jobs=-1         # Use all CPU cores\n",
    ")\n",
    "```\n",
    "\n",
    "#### B. Support Vector Machine (SVM)\n",
    "```python\n",
    "svm_model = SVC(\n",
    "    kernel='rbf',      # Radial basis function kernel\n",
    "    C=10,              # Regularization parameter\n",
    "    gamma='scale',     # Kernel coefficient\n",
    "    probability=True,  \n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "#### C. XGBoost\n",
    "```python\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,      # Number of boosting rounds\n",
    "    learning_rate=0.1,     # Step size shrinkage\n",
    "    max_depth=7,          # Maximum tree depth\n",
    "    subsample=0.8,        # Fraction of samples used\n",
    "    colsample_bytree=0.8, # Fraction of features used\n",
    "    objective='multi:softprob',\n",
    "    num_class=len(encoded_classes)\n",
    ")\n",
    "```\n",
    "\n",
    "### 4. Model Evaluation\n",
    "Performs comprehensive evaluation using multiple metrics:\n",
    "\n",
    "1. **Accuracy Scores**\n",
    "   - Calculates overall accuracy for each model\n",
    "   - Compares models to identify best performer\n",
    "\n",
    "2. **Detailed Classification Reports**\n",
    "   - Precision\n",
    "   - Recall\n",
    "   - F1-score\n",
    "   - Support for each sleep stage\n",
    "\n",
    "3. **Confusion Matrices**\n",
    "   - Visualizes prediction patterns\n",
    "   - Shows misclassification patterns between stages\n",
    "\n",
    "4. **Learning Curves**\n",
    "   - Plots training vs validation scores\n",
    "   - Helps identify overfitting/underfitting\n",
    "   - Shows if more data would help improve performance\n",
    "\n",
    "### 5. Model Persistence\n",
    "Saves the best performing model along with its preprocessing components:\n",
    "- Model object\n",
    "- Feature scaler\n",
    "- Feature selector\n",
    "- Feature names\n",
    "- Selected feature list\n",
    "\n",
    "### 6. Visualization\n",
    "Creates multiple plots for analysis:\n",
    "- Feature importance bar chart\n",
    "- Confusion matrices for each model\n",
    "- Model accuracy comparison bar chart\n",
    "- Learning curve for best model\n",
    "\n",
    "This implementation follows machine learning best practices:\n",
    "- Uses cross-validation for robust evaluation\n",
    "- Implements feature selection to prevent overfitting\n",
    "- Compares multiple algorithms\n",
    "- Provides comprehensive performance metrics\n",
    "- Includes visualization for interpretation\n",
    "- Saves model and preprocessing pipeline for deployment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification model\n",
    "print(\"\\nBuilding sleep stage classification models...\")\n",
    "import pandas as pd\n",
    "\n",
    "# Load the DataFrame if not already in memory\n",
    "try:\n",
    "    feature_df\n",
    "except NameError:\n",
    "    feature_df = pd.read_csv('eeg_features.csv')\n",
    "\n",
    "# Remove any NaN values\n",
    "feature_df = feature_df.dropna()\n",
    "\n",
    "# Remove rows where sleep_stage = 1 (empty label) or any invalid stage\n",
    "valid_stages = [0, 2, 3, 4, 5]  # REM, N3, N2, N1, Wake\n",
    "feature_df = feature_df[feature_df['sleep_stage'].isin(valid_stages)]\n",
    "\n",
    "# Define X (features) and y (target labels)\n",
    "X = feature_df.drop(['timestamp', 'sleep_stage'], axis=1)\n",
    "y = feature_df['sleep_stage']\n",
    "\n",
    "# Print the number of samples for each sleep stage\n",
    "print(\"Sleep stage distribution:\")\n",
    "stage_counts = y.value_counts().sort_index()\n",
    "stage_mapping = {0: 'REM', 2: 'N3', 3: 'N2', 4: 'N1', 5: 'Wake'}\n",
    "for stage, count in stage_counts.items():\n",
    "    stage_name = stage_mapping.get(stage, f'Unknown {stage}')\n",
    "    print(f\"  {stage_name}: {count} samples\")\n",
    "\n",
    "# Use a LabelEncoder to transform non-consecutive labels to consecutive integers (0, 1, 2, 3, 4)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Store the original class names in the same order as the encoded labels\n",
    "encoded_classes = [stage_mapping[stage] for stage in label_encoder.classes_]\n",
    "\n",
    "# Create training and validation sets with 40:60 ratio\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.6, random_state=42, stratify=y_encoded)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "\n",
    "# Create feature selector to use the most important features\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# First, let's check for feature importance using a preliminary Random Forest\n",
    "preliminary_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "preliminary_rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = preliminary_rf.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.bar(range(len(indices[:20])), importances[indices[:20]], align='center')\n",
    "plt.xticks(range(len(indices[:20])), [feature_names[i] for i in indices[:20]], rotation=90)\n",
    "plt.title('Top 20 Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.savefig(get_output_path('feature_importance.png'))\n",
    "plt.show()\n",
    "\n",
    "# Select the top 20 features for models to prevent overfitting\n",
    "selector = SelectFromModel(preliminary_rf, threshold=-np.inf, max_features=20)\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_val_selected = selector.transform(X_val)\n",
    "\n",
    "# Get names of selected features\n",
    "selected_feature_indices = selector.get_support(indices=True)\n",
    "selected_feature_names = [feature_names[i] for i in selected_feature_indices]\n",
    "print(\"\\nTop 20 selected features:\")\n",
    "for i, feature in enumerate(selected_feature_names):\n",
    "    print(f\"{i+1}. {feature}\")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "X_val_scaled = scaler.transform(X_val_selected)\n",
    "\n",
    "# 1. Random Forest Classifier\n",
    "print(\"\\nTraining Random Forest classifier...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "rf_pred = rf_model.predict(X_val_scaled)\n",
    "rf_accuracy = accuracy_score(y_val, rf_pred)\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
    "\n",
    "# 2. Support Vector Machine\n",
    "print(\"\\nTraining SVM classifier...\")\n",
    "svm_model = SVC(kernel='rbf', C=10, gamma='scale', probability=True, random_state=42)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "svm_pred = svm_model.predict(X_val_scaled)\n",
    "svm_accuracy = accuracy_score(y_val, svm_pred)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "\n",
    "# 3. XGBoost\n",
    "print(\"\\nTraining XGBoost classifier...\")\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=7,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='multi:softprob',\n",
    "    num_class=len(encoded_classes),  # Use the correct number of classes\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "xgb_pred = xgb_model.predict(X_val_scaled)\n",
    "xgb_accuracy = accuracy_score(y_val, xgb_pred)\n",
    "print(f\"XGBoost Accuracy: {xgb_accuracy:.4f}\")\n",
    "\n",
    "# Compare model accuracies\n",
    "model_accuracies = {\n",
    "    'Random Forest': rf_accuracy,\n",
    "    'SVM': svm_accuracy,\n",
    "    'XGBoost': xgb_accuracy\n",
    "}\n",
    "\n",
    "# Find the best model\n",
    "best_model_name = max(model_accuracies, key=model_accuracies.get)\n",
    "best_model_accuracy = model_accuracies[best_model_name]\n",
    "print(f\"\\nBest model: {best_model_name} with accuracy {best_model_accuracy:.4f}\")\n",
    "\n",
    "# Detailed classification report for all models\n",
    "print(\"\\n--- Random Forest Classification Report ---\")\n",
    "print(classification_report(y_val, rf_pred, target_names=encoded_classes))\n",
    "\n",
    "print(\"\\n--- SVM Classification Report ---\")\n",
    "print(classification_report(y_val, svm_pred, target_names=encoded_classes))\n",
    "\n",
    "print(\"\\n--- XGBoost Classification Report ---\")\n",
    "print(classification_report(y_val, xgb_pred, target_names=encoded_classes))\n",
    "\n",
    "# Plot confusion matrices\n",
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=encoded_classes,\n",
    "                yticklabels=encoded_classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(get_output_path(f'{title.replace(\" \", \"_\").lower()}.png'))\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrices for all models\n",
    "plot_confusion_matrix(y_val, rf_pred, 'Random Forest Confusion Matrix')\n",
    "plot_confusion_matrix(y_val, svm_pred, 'SVM Confusion Matrix')\n",
    "plot_confusion_matrix(y_val, xgb_pred, 'XGBoost Confusion Matrix')\n",
    "\n",
    "# Bar plot comparing model accuracies\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(model_accuracies.keys(), model_accuracies.values())\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "for i, (model, acc) in enumerate(model_accuracies.items()):\n",
    "    plt.text(i, acc + 0.01, f'{acc:.4f}', ha='center')\n",
    "plt.tight_layout()\n",
    "plt.savefig(get_output_path('model_comparison.png'))\n",
    "plt.show()\n",
    "\n",
    "# Save the best model\n",
    "best_model = None\n",
    "if best_model_name == 'Random Forest':\n",
    "    best_model = rf_model\n",
    "elif best_model_name == 'SVM':\n",
    "    best_model = svm_model\n",
    "else:  # XGBoost\n",
    "    best_model = xgb_model\n",
    "\n",
    "import pickle\n",
    "with open(get_output_path('best_sleep_classifier.pkl'), 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'model': best_model,\n",
    "        'scaler': scaler,\n",
    "        'selector': selector,\n",
    "        'feature_names': feature_names,\n",
    "        'selected_features': selected_feature_names\n",
    "    }, f)\n",
    "\n",
    "print(f\"\\nBest model saved as '@Output/best_sleep_classifier.pkl'\")\n",
    "\n",
    "# Generate learning curves to evaluate if more data would help\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=5,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                        train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                        test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "# Plot learning curve for the best model\n",
    "print(f\"\\nGenerating learning curve for {best_model_name}...\")\n",
    "\n",
    "# Use a smaller subset for learning curve to speed up computation\n",
    "X_subset = X_train_scaled[:min(1000, len(X_train_scaled))]\n",
    "y_subset = y_train[:min(1000, len(y_train))]\n",
    "\n",
    "if best_model_name == 'Random Forest':\n",
    "    lc_model = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)\n",
    "elif best_model_name == 'SVM':\n",
    "    lc_model = SVC(kernel='rbf', C=10, gamma='scale', random_state=42)\n",
    "else:  # XGBoost\n",
    "    lc_model = xgb.XGBClassifier(n_estimators=50, learning_rate=0.1, max_depth=7, random_state=42)\n",
    "    \n",
    "plot_learning_curve(lc_model, f'Learning Curve ({best_model_name})', \n",
    "                    X_subset, y_subset[:len(X_subset)], ylim=(0.5, 1.01), cv=5, n_jobs=-1)\n",
    "plt.savefig(get_output_path('learning_curve.png'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
